\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{LSTM}
\citation{Sutskever}
\citation{Parsing}
\citation{Graves}
\citation{Sutskever}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Memory Models}{1}{subsection.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Other Model Architectures}{1}{subsection.1.2}}
\citation{DBLP:journals/corr/ChungGCB15}
\@writefile{toc}{\contentsline {section}{\numberline {2}Recurrent Neural Networks}{2}{section.2}}
\newlabel{sec:nn}{{2}{2}{Recurrent Neural Networks}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}LSTM}{2}{subsection.2.1}}
\newlabel{sec:nn/lstm}{{2.1}{2}{LSTM}{subsection.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A demonstration of the interactions within an LSTM cell. Here we can see clearly how the input and output control the respective features of the cell: the closer to zero these vectors are, the smaller the amount of information which can travel through the dot product.}}{2}{figure.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Gated Recurrent Unit}{2}{subsection.2.2}}
\newlabel{sec:nn/gru}{{2.2}{2}{Gated Recurrent Unit}{subsection.2.2}{}}
\citation{peepholeconnections}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Peephole Connections}{3}{subsection.2.3}}
\newlabel{sec:nn/peephole}{{2.3}{3}{Peephole Connections}{subsection.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Here we can clearly see how all gates now have direct interaction with the memory state.}}{3}{figure.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Datasets}{3}{section.3}}
\newlabel{sec:data}{{3}{3}{Datasets}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Energy Load}{3}{subsection.3.1}}
\newlabel{sec:data/energy}{{3.1}{3}{Energy Load}{subsection.3.1}{}}
\citation{energy_kaggle}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Household Energy Consumption}{4}{subsection.3.2}}
\newlabel{sec:data/house}{{3.2}{4}{Household Energy Consumption}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Prediction of Energy Loads}{4}{section.4}}
\newlabel{sec:energy}{{4}{4}{Prediction of Energy Loads}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Features and Regression Model}{5}{subsection.4.1}}
\newlabel{sec:energy/reg}{{4.1}{5}{Features and Regression Model}{subsection.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Predicted and actual energy loads for two different weeks and zones. Prediction is done by regression model.}}{5}{figure.3}}
\newlabel{fig:energy/reg}{{3}{5}{Predicted and actual energy loads for two different weeks and zones. Prediction is done by regression model}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Feed-Forward Neural Network}{5}{subsection.4.2}}
\newlabel{sec:energy/nn}{{4.2}{5}{Feed-Forward Neural Network}{subsection.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Architecture of feed-forward neural network.}}{6}{figure.4}}
\newlabel{fig:energy/nn_diag}{{4}{6}{Architecture of feed-forward neural network}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Relative error as function of learning epoch for training, validation and test data.}}{6}{figure.5}}
\newlabel{fig:energy/nn/learn}{{5}{6}{Relative error as function of learning epoch for training, validation and test data}{figure.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Recurrent Neural Network}{6}{subsection.4.3}}
\newlabel{sec:energy/rnn}{{4.3}{6}{Recurrent Neural Network}{subsection.4.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Architecture of RNN with feed-forward neural network.}}{7}{figure.6}}
\newlabel{fig:energy/rnn_diag}{{6}{7}{Architecture of RNN with feed-forward neural network}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Error of the RNN-dense network as function of training epoch for training, validation and test sets.}}{7}{figure.7}}
\newlabel{fig:energy/rnn_learn}{{7}{7}{Error of the RNN-dense network as function of training epoch for training, validation and test sets}{figure.7}{}}
\bibcite{DBLP:journals/corr/ChungGCB15}{1}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Architecture of double-RNN with feed-forward neural network.}}{8}{figure.8}}
\newlabel{fig:energy/rnn2_diag}{{8}{8}{Architecture of double-RNN with feed-forward neural network}{figure.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Prediction of Household Energy Consumption}{8}{section.5}}
\newlabel{sec:house}{{5}{8}{Prediction of Household Energy Consumption}{section.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusions}{8}{section.6}}
\newlabel{sec:conclusions}{{6}{8}{Conclusions}{section.6}{}}
\bibcite{peepholeconnections}{2}
\bibcite{Sutskever}{3}
\bibcite{Graves}{4}
\bibcite{Parsing}{5}
\bibcite{LSTM}{6}
\bibcite{energy_kaggle}{7}
